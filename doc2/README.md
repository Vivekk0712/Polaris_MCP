ğŸ§  AutoML Multi-Agent System (MCP + Supabase + PyTorch + GCP)
ğŸ“ Team Size: 4 Members
Each member builds one independent AI Agent.
________________________________________
ğŸš€ Overview
This project is a multi-agent AutoML pipeline built using:
â€¢	MCP Server (for orchestration and chatbot integration)
â€¢	Supabase (for centralized database + message storage)
â€¢	Google Cloud Storage (GCP) (for dataset & model storage)
â€¢	PyTorch (for model training & evaluation)
â€¢	Gemini LLM (for reasoning in Planner Agent)
Each agent handles one stage of the ML workflow â€” from dataset discovery to final evaluation â€” and all communication happens through Supabase tables (no direct API calls between agents).
________________________________________
âš™ï¸ System Architecture
User
 â†“
MCP Server (chatbot)
 â”œâ”€â”€ Planner Agent â†’ creates project plan
 â”œâ”€â”€ Dataset Agent â†’ fetches & uploads dataset
 â”œâ”€â”€ Training Agent â†’ trains model locally
 â””â”€â”€ Evaluation Agent â†’ evaluates trained model
 â†“
Supabase (Database)
 â†“
GCP Bucket (Storage)
________________________________________
ğŸ§© Agent Responsibilities
Agent	Member	Description
ğŸ§  Planner Agent	Member 1	Interprets user intent (via Gemini), creates project plan in Supabase (projects table).
ğŸ“¦ Dataset Agent	Member 2	Authenticates Kaggle, downloads dataset, uploads to GCP, updates datasets table.
âš™ï¸ Training Agent	Member 3	Downloads dataset from GCP, trains PyTorch model locally, uploads model to GCP, updates models.
ğŸ“Š Evaluation Agent	Member 4	Evaluates trained model using test data, logs accuracy and metrics, marks project as completed.
________________________________________
ğŸ§± Database Schema (Supabase)
Core Tables
create table if not exists projects (
  id uuid primary key default gen_random_uuid(),
  user_id uuid references users(id) on delete cascade,
  name text not null,
  task_type text not null,
  framework text default 'pytorch',
  dataset_source text default 'kaggle',
  search_keywords text[],
  status text default 'draft',
  metadata jsonb default '{}'::jsonb,
  created_at timestamptz default now(),
  updated_at timestamptz default now()
);

create table if not exists datasets (
  id uuid primary key default gen_random_uuid(),
  project_id uuid references projects(id) on delete cascade,
  name text,
  gcs_url text,
  size text,
  source text default 'kaggle',
  created_at timestamptz default now()
);

create table if not exists models (
  id uuid primary key default gen_random_uuid(),
  project_id uuid references projects(id) on delete cascade,
  name text,
  framework text default 'pytorch',
  gcs_url text,
  accuracy numeric,
  metadata jsonb default '{}'::jsonb,
  created_at timestamptz default now()
);

create table if not exists agent_logs (
  id uuid primary key default gen_random_uuid(),
  project_id uuid references projects(id) on delete cascade,
  agent_name text,
  message text,
  log_level text default 'info',
  created_at timestamptz default now()
);
Existing Chat Tables (already in your MCP)
users, messages, embeddings
________________________________________
â˜ï¸ GCP Bucket Structure
gs://automl-datasets/
 â”œâ”€â”€ raw/
 â”‚    â”œâ”€â”€ plantvillage.zip
 â”‚    â”œâ”€â”€ chestxray.zip
 â”œâ”€â”€ models/
 â”‚    â”œâ”€â”€ plantvillage_model.pth
 â””â”€â”€ temp/
      â”œâ”€â”€ intermediate/
Naming convention:
â€¢	Dataset files: raw/{dataset_name}.zip
â€¢	Models: models/{project_name}_model.pth
________________________________________
âš¡ Workflow Summary
Step	Agent	Input	Output	Supabase Status
1ï¸âƒ£	Planner Agent	User message	JSON project plan	pending_dataset
2ï¸âƒ£	Dataset Agent	Project ID	GCS dataset URL	pending_training
3ï¸âƒ£	Training Agent	Dataset URL	GCS model file	pending_evaluation
4ï¸âƒ£	Evaluation Agent	Model + dataset	Accuracy + metrics	completed
All coordination happens through projects.status.
________________________________________
ğŸ§© MCP Server Integration
Folder Structure
AutoML-MCP-Agents/
 â”œâ”€â”€ mcp_server/
 â”‚    â””â”€â”€ main.py
 â”œâ”€â”€ agents/
 â”‚    â”œâ”€â”€ planner/
 â”‚    â”‚    â”œâ”€â”€ main.py
 â”‚    â”‚    â””â”€â”€ architecture.md
 â”‚    â”œâ”€â”€ dataset/
 â”‚    â”‚    â”œâ”€â”€ main.py
 â”‚    â”‚    â””â”€â”€ architecture.md
 â”‚    â”œâ”€â”€ training/
 â”‚    â”‚    â”œâ”€â”€ main.py
 â”‚    â”‚    â””â”€â”€ architecture.md
 â”‚    â””â”€â”€ evaluation/
 â”‚         â”œâ”€â”€ main.py
 â”‚         â””â”€â”€ architecture.md
 â”œâ”€â”€ README.md  â† (this file)
 â”œâ”€â”€ requirements.txt
 â””â”€â”€ .env
________________________________________
ğŸ§  MCP Configuration (Example)
In mcp.yaml or config.json:
tools:
  - name: planner
    path: ./agents/planner/main.py
  - name: dataset
    path: ./agents/dataset/main.py
  - name: training
    path: ./agents/training/main.py
  - name: evaluation
    path: ./agents/evaluation/main.py
Each tool registers itself when the MCP Server starts.
________________________________________
ğŸ”‘ Environment Variables
Create a .env file at root:
SUPABASE_URL=
SUPABASE_KEY=
GCP_BUCKET_NAME=
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service_account.json
GEMINI_API_KEY=
MCP_API_KEY=
LOG_LEVEL=INFO
Each agent reads the same env file (shared configs).
________________________________________
ğŸ§° Local Setup Instructions
1ï¸âƒ£ Clone the repo
git clone https://github.com/<your-team>/AutoML-MCP-Agents.git
cd AutoML-MCP-Agents
2ï¸âƒ£ Create a Python environment
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)
3ï¸âƒ£ Install dependencies
pip install -r requirements.txt
4ï¸âƒ£ Run the MCP server
cd mcp_server
uvicorn main:app --reload
5ï¸âƒ£ Run an agent (example)
cd ../agents/training
python main.py
Each agent can be run locally or inside a lightweight Docker container.
________________________________________
ğŸ§© How Agents Communicate
All agents are stateless and interact through Supabase:
â€¢	Planner inserts â†’ projects
â€¢	Dataset reads â†’ inserts â†’ updates status
â€¢	Training reads â†’ inserts model â†’ updates status
â€¢	Evaluation reads â†’ updates metrics â†’ finalizes project
________________________________________
ğŸ§¾ Testing End-to-End
Stage	Input	Expected Outcome
ğŸ§  Planner	â€œTrain a PyTorch model for tomato leavesâ€	Project appears in Supabase with pending_dataset.
ğŸ“¦ Dataset	Kaggle key uploaded	Dataset uploaded to GCP; status â†’ pending_training.
âš™ï¸ Training	Trigger by MCP	Model trained locally, uploaded to GCP; status â†’ pending_evaluation.
ğŸ“Š Evaluation	Auto-trigger	Metrics computed, status â†’ completed.
âœ… Output		Chatbot shows: â€œModel accuracy 93.8%. Project complete!â€
________________________________________
ğŸ§© Team Member Division
Member	Agent	Key Skills Used
1ï¸âƒ£	Planner Agent	LLM integration, Supabase schema design
2ï¸âƒ£	Dataset Agent	Kaggle API, GCP uploads, data management
3ï¸âƒ£	Training Agent	PyTorch model training, file upload
4ï¸âƒ£	Evaluation Agent	Model evaluation, metric computation
________________________________________
ğŸ” Security Guidelines
â€¢	Never store user kaggle.json beyond the session.
â€¢	Restrict Supabase service keys (write-only for agents).
â€¢	Use least-privilege service accounts for GCP uploads.
â€¢	Validate all Supabase input before insert/update.
â€¢	Ensure model training runs locally in isolated environment (no untrusted code).
________________________________________
ğŸ§­ Future Enhancements
â€¢	Add Auto Hyperparameter Tuner Agent.
â€¢	Introduce Model Comparison Dashboard (Supabase + Streamlit).
â€¢	Add Docker Compose file for one-click setup.
â€¢	Add RAG Agent later (to remember past model results).
â€¢	Enable optional GPU cloud training via RunPod or Vertex AI.
________________________________________
âœ… End-to-End Summary
Layer	Description
Frontend	MCP Chatbot for user interaction
Middleware	MCP Server routes requests to correct agent
Backend	4 independent AI Agents (Planner, Dataset, Training, Evaluation)
Database	Supabase stores metadata, messages, logs
Storage	GCP bucket stores large datasets & trained models
Execution	Local PyTorch for training & evaluation
Output	Metrics + accuracy summary displayed in chat
________________________________________
ğŸ“¸ Example Final Flow
User: "Train a PyTorch model for plant disease detection"
 â†“
Planner Agent â†’ Creates project plan
 â†“
Dataset Agent â†’ Fetches dataset from Kaggle â†’ Uploads to GCP
 â†“
Training Agent â†’ Downloads dataset â†’ Trains model â†’ Uploads .pth to GCP
 â†“
Evaluation Agent â†’ Evaluates model â†’ Updates Supabase
 â†“
Chatbot â†’ "âœ… Training complete. Accuracy: 93.8%."

