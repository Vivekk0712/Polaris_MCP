üìä Evaluation Agent ‚Äî Architecture Document
(Team Member 4 Ownership)
________________________________________
üß≠ Overview
The Evaluation Agent validates and scores trained models generated by the Training Agent.
It:
1.	Downloads the trained .pth model and dataset from the GCP bucket.
2.	Runs inference on the test split using PyTorch.
3.	Computes accuracy, precision, recall, and F1 scores.
4.	Logs results to the models table in Supabase.
5.	Sets projects.status = 'completed' once evaluation is done.
6.	Sends a chat message back to the user through MCP to summarize the results.
________________________________________
‚öôÔ∏è Responsibilities
Task	Description
Model Retrieval	Download model weights from GCP bucket.
Dataset Access	Download test subset from the same bucket.
Evaluation	Load model in PyTorch, run predictions on test images.
Metric Computation	Calculate accuracy, precision, recall, F1.
Result Storage	Update Supabase models row and projects status.
User Notification	Send evaluation summary to chat history.
________________________________________
üß± Technology Stack
‚Ä¢	Language: Python 3.10+
‚Ä¢	Framework: FastAPI
‚Ä¢	ML Lib: PyTorch + Torchvision
‚Ä¢	Metrics: scikit-learn (classification_report)
‚Ä¢	Storage: Google Cloud Storage (GCS)
‚Ä¢	Database: Supabase (Postgres via supabase-py)
‚Ä¢	Orchestration: MCP Server
________________________________________
üóÉÔ∏è Supabase Tables Used
Table	Action	Purpose
projects	read/update	Identify project and set status completed.
models	read/update	Fetch model path, store metrics JSON.
datasets	read	Fetch test dataset location.
messages	insert	Post summary to chat.
agent_logs	insert	Evaluation progress + errors.
________________________________________
üß© Workflow
1Ô∏è‚É£ Trigger
When the Training Agent updates projects.status = 'pending_evaluation',
the MCP Server calls:
POST /agents/evaluation/start
{ "project_id": "<uuid>" }
________________________________________
2Ô∏è‚É£ Fetch Metadata
project = supabase.table("projects").select("*").eq("id", project_id).execute().data[0]
model = supabase.table("models").select("*").eq("project_id", project_id).execute().data[0]
dataset = supabase.table("datasets").select("*").eq("project_id", project_id).execute().data[0]
Retrieve:
‚Ä¢	model["gcs_url"] ‚Üí trained model path
‚Ä¢	dataset["gcs_url"] ‚Üí test data path
________________________________________
3Ô∏è‚É£ Download Files from GCP
from google.cloud import storage

def download_from_gcp(gcs_url, dest):
  client = storage.Client()
  bucket_name, blob_name = parse_gcs_url(gcs_url)
  bucket = client.bucket(bucket_name)
  bucket.blob(blob_name).download_to_filename(dest)
Download model ‚Üí model.pth
Download dataset ‚Üí data/test/
________________________________________
4Ô∏è‚É£ Evaluate Model with PyTorch
import torch
from torchvision import datasets, transforms, models
from sklearn.metrics import classification_report, accuracy_score
import json, numpy as np

def evaluate_model(data_dir="data/test", model_path="model.pth", num_classes=5):
  transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
  ])
  test_ds = datasets.ImageFolder(root=data_dir, transform=transform)
  test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32)
  
  model = models.resnet18()
  model.fc = torch.nn.Linear(model.fc.in_features, num_classes)
  model.load_state_dict(torch.load(model_path))
  model.eval()

  y_true, y_pred = [], []
  with torch.no_grad():
    for imgs, labels in test_loader:
      outputs = model(imgs)
      preds = torch.argmax(outputs, dim=1)
      y_true.extend(labels.numpy())
      y_pred.extend(preds.numpy())

  report = classification_report(y_true, y_pred, output_dict=True)
  acc = accuracy_score(y_true, y_pred)
  torch.save(model.state_dict(), "evaluated_model.pth")

  return acc, report
________________________________________
5Ô∏è‚É£ Update Supabase
acc, metrics = evaluate_model()
supabase.table("models").update({
  "accuracy": acc,
  "metadata": metrics
}).eq("id", model["id"]).execute()

supabase.table("projects").update({
  "status": "completed"
}).eq("id", project_id).execute()
________________________________________
6Ô∏è‚É£ Send Summary to User
Add to messages table:
supabase.table("messages").insert({
  "user_id": project["user_id"],
  "role": "assistant",
  "content": f"‚úÖ Model evaluation complete! Accuracy: {acc:.2%}"
}).execute()
________________________________________
üßÆ Environment Variables
SUPABASE_URL=
SUPABASE_KEY=
GCP_BUCKET_NAME=
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service_account.json
MCP_API_KEY=
LOG_LEVEL=INFO
________________________________________
üß† API Endpoints (Agent)
Method	Path	Description
POST	/agents/evaluation/start	Run evaluation for given project ID.
GET	/agents/evaluation/status/{project_id}	Show metrics and progress.
GET	/health	Health check.
________________________________________
üß∞ Code Skeleton
from fastapi import FastAPI
from supabase import create_client
import os

app = FastAPI()
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

@app.post("/agents/evaluation/start")
def start_eval(payload: dict):
  project_id = payload["project_id"]
  project = supabase.table("projects").select("*").eq("id", project_id).execute().data[0]
  model = supabase.table("models").select("*").eq("project_id", project_id).execute().data[0]
  dataset = supabase.table("datasets").select("*").eq("project_id", project_id).execute().data[0]

  download_from_gcp(model["gcs_url"], "model.pth")
  download_from_gcp(dataset["gcs_url"], "data.zip")
  # unzip data.zip to data/test

  acc, report = evaluate_model("data/test", "model.pth", num_classes=5)

  supabase.table("models").update({
    "accuracy": acc,
    "metadata": report
  }).eq("id", model["id"]).execute()

  supabase.table("projects").update({"status": "completed"}).eq("id", project_id).execute()

  supabase.table("messages").insert({
    "user_id": project["user_id"],
    "role": "assistant",
    "content": f"‚úÖ Model evaluation complete! Accuracy {acc:.2%}"
  }).execute()

  return {"success": True, "accuracy": acc}
________________________________________
üß© Integration with MCP Server
1.	Add in mcp.yaml:
tools:
  - name: evaluation
    path: ./agents/evaluation/main.py
2.	Triggered automatically after pending_training ‚Üí pending_evaluation.
3.	Updates Supabase ‚Üí chatbot shows final results.
________________________________________
üßæ Testing Checklist
Test	Expected Result
Valid model & dataset	Accuracy + metrics stored in Supabase.
Corrupt model	Error logged + project status = failed.
Large test dataset	Evaluates batch-wise without crash.
Upload permissions	Verified GCP auth for downloads.
Supabase update	projects.status ‚Üí completed.
Chat message	Final results visible to user.
________________________________________
‚úÖ Deliverables for Member 4
‚Ä¢	 FastAPI service (/agents/evaluation)
‚Ä¢	 PyTorch model loading & metrics calculation
‚Ä¢	 Supabase update for results and status
‚Ä¢	 GCP download integration
‚Ä¢	 Log progress to agent_logs
‚Ä¢	 Demo run: project transitions pending_evaluation ‚Üí completed

